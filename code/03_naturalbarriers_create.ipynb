{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polygonization by rivers & railways - get raw data; filter by tag; polygonize\n",
    "\n",
    "In this notebook, for each city, we\n",
    "* load the city boundary polygon\n",
    "* download and save raw OSM data (features) on rivers/railways for this polygon\n",
    "* load the info which tags to keep (derived from **manual exploration** of each data set, cf. `03_naturalbarriers_explore.ipynb`)\n",
    "* process the data set (dropping unwanted tags), merge, and polygonize\n",
    "* save polygons as output (\"fragmentation by rivers/railways/borders\" in each city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "# import sys\n",
    "# import argparse\n",
    "# import random\n",
    "# from time import time\n",
    "# import numpy as np\n",
    "import pandas as pd\n",
    "# import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "# from shapely.geometry import LineString, Point, MultiPoint, MultiLineString, Polygon\n",
    "# from tqdm import tqdm\n",
    "# from geopy.distance import distance, geodesic, great_circle\n",
    "import osmnx as ox\n",
    "# import networkx as nx\n",
    "# import scipy.stats\n",
    "# from scipy.stats import ks_2samp\n",
    "# import sklearn\n",
    "# import igraph\n",
    "# from igraph import Graph\n",
    "\n",
    "# from random import choice\n",
    "# from bisect import bisect_left\n",
    "# import copy\n",
    "\n",
    "# from functools import partial\n",
    "# import pyproj\n",
    "# from pyproj import Geod\n",
    "# from pyproj.crs import ProjectedCRS\n",
    "# from pyproj.crs.coordinate_operation import AzimuthalEquidistantConversion\n",
    "# from shapely.ops import transform\n",
    "\n",
    "import yaml\n",
    "# from utils import *\n",
    "import shapely\n",
    "# ox.__version__\n",
    "import momepy\n",
    "import contextily as cx\n",
    "import folium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Def tiny functions**\n",
    "(plots for sanity check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_polygons(polygons, natbar, folder_plots, city_name):\n",
    "    fig, ax = plt.subplots(1,1, figsize = (20,20))\n",
    "    polygons.plot(ax=ax, color = \"#F7F7F7\", alpha = .5)\n",
    "    polygons.boundary.plot(ax=ax, color = \"black\", lw = 1, linestyle = \"dotted\")\n",
    "    natbar.plot(ax=ax, column=\"barrier_type\", legend = True, lw = 10, alpha = .2)\n",
    "    xlims = list(natbar[natbar[\"barrier_type\"]==\"city_border\"].bounds[[\"minx\", \"maxx\"]].values[0])\n",
    "    ylims = list(natbar[natbar[\"barrier_type\"]==\"city_border\"].bounds[[\"miny\", \"maxy\"]].values[0])\n",
    "    ax.set_xlim(xlims)\n",
    "    ax.set_ylim(ylims)\n",
    "    ax.set_axis_off()\n",
    "    ax.set_title(city_name, fontsize = 20)\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(folder_plots + f\"{city_name}.png\", dpi = 300, bbox_inches = \"tight\")\n",
    "    plt.close()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_crs = \"epsg:9311\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create subfolders for output data\n",
    "folders = [\n",
    "    \"../data/natural_barriers/\",\n",
    "    \"../data/natural_barriers/raw/\",\n",
    "    \"../data/natural_barriers/polygonized/\"\n",
    "    \"../data/natural_barriers/plots/\"\n",
    "]\n",
    "for folder in folders:\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "# define folder paths\n",
    "folder_osm = \"../data/natural_barriers/raw/\"\n",
    "folder_poly = \"../data/natural_barriers/polygonized/\"\n",
    "folder_plots = \"../data/natural_barriers/plots/\"\n",
    "\n",
    "barrier_types = [\"railway\", \"waterway\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in and format CBSA table to loop through\n",
    "cbsacode_file = '../data/cbsacode.csv'\n",
    "df_cbsacodes = pd.read_csv(cbsacode_file)\n",
    "df_cbsacodes = df_cbsacodes[[\"cbsacode\", \"name\", \"full_name\", \"geometry\", \"west\", \"south\", \"east\", \"north\"]]\n",
    "# convert text to shapely Polygon\n",
    "df_cbsacodes[\"geometry\"] = df_cbsacodes.geometry.apply(lambda x: shapely.from_wkt(x))\n",
    "# convert to geodataframe\n",
    "gdf_cbsacodes = gpd.GeoDataFrame(df_cbsacodes, crs = \"EPSG:4326\")\n",
    "gdf_cbsacodes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_cbsacodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and save data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Download and save OSM data\n",
    "\n",
    "# initialize dict where we will save all data\n",
    "data_dict = {}\n",
    "\n",
    "for _, row in gdf_cbsacodes.iterrows():\n",
    "\n",
    "    cbsacode = row.cbsacode\n",
    "    city_name = row[\"name\"]\n",
    "    geom = row.geometry\n",
    "\n",
    "    print(f\"{city_name}:\")\n",
    "\n",
    "    data_dict[city_name] = {} \n",
    "\n",
    "    for barrier_type in barrier_types:\n",
    "        \n",
    "        barrier_path = folder_osm + f'{cbsacode}_{barrier_type}.gpkg'\n",
    "        \n",
    "        if os.path.exists(barrier_path):\n",
    "            print(f\"\\t file found, loading OSM data: {barrier_type}\")\n",
    "            gdf = gpd.read_file(barrier_path)\n",
    "        else:\n",
    "            print(f\"\\t file not found, downloading OSM data: {barrier_type}\")\n",
    "            gdf = ox.features_from_polygon(\n",
    "                polygon=geom,\n",
    "                tags = {\n",
    "                    barrier_type:True, \n",
    "                }\n",
    "            )\n",
    "            gdf = gdf.explode(ignore_index=True)\n",
    "            gdf = gdf[[\"geometry\",barrier_type]]\n",
    "            gdf.to_file(barrier_path)\n",
    "            \n",
    "        data_dict[city_name][barrier_type] = gdf\n",
    "        del gdf\n",
    "\n",
    "        print(\"\\t done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*******\n",
    "*******\n",
    "\n",
    "# Manual verification\n",
    "\n",
    "(cf. `03_naturalbarriers_explore.ipynb`)\n",
    "\n",
    "*******\n",
    "*******\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load tag dict** (derived manually)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"barriertags.yml\", \"r\") as file:\n",
    "    tags = yaml.load(file, Loader=yaml.FullLoader)\n",
    "tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**reduce data sets** accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for city_name, tag_dict in tags.items():    \n",
    "\n",
    "    gdfs = []\n",
    "\n",
    "    cbsacode = gdf_cbsacodes[gdf_cbsacodes[\"name\"]==city_name].cbsacode.values[0]\n",
    "    \n",
    "    natbar_path = folder_poly + f\"{cbsacode}_natbar.gpkg\"\n",
    "    polygon_path = folder_poly + f\"{cbsacode}_polygons.gpkg\"\n",
    "\n",
    "    # do this only if not done yet\n",
    "    if not (os.path.exists(natbar_path) and os.path.exists(polygon_path)):\n",
    "        print(\"Runing for\", city_name)\n",
    "        city_border = gpd.GeoDataFrame(\n",
    "            {\n",
    "                \"geometry\": [gdf_cbsacodes[gdf_cbsacodes[\"name\"]==city_name].boundary.values[0]],\n",
    "                \"barrier_type\": \"city_border\"\n",
    "            },\n",
    "            crs = gdf_cbsacodes.crs\n",
    "        )\n",
    "        \n",
    "        gdfs.append(city_border)\n",
    "\n",
    "        for barrier_type in barrier_types:\n",
    "            gdf = data_dict[city_name][barrier_type].copy()\n",
    "            gdf = gdf[(gdf.geom_type==\"LineString\")&(gdf[barrier_type]).isin(tag_dict[barrier_type])]\n",
    "            gdf[\"barrier_type\"] = gdf[barrier_type]\n",
    "            del gdf[barrier_type]\n",
    "            # data_dict[city_name][barrier_type] = gdf\n",
    "            gdfs.append(gdf)\n",
    "\n",
    "        # now we have a list of 3 gdfs: city border, railway, waterway. concat them all:\n",
    "        # and save to file - these are the polygon outlines\n",
    "        natbar = pd.concat(gdfs).reset_index(drop=True)\n",
    "        natbar.to_file(folder_poly + f\"{cbsacode}_natbar.gpkg\", index = False)\n",
    "\n",
    "        # and polygonize & save\n",
    "        polygons = momepy.FaceArtifacts(natbar).polygons[[\"geometry\"]].set_crs(natbar.crs)\n",
    "        polygons.to_file(folder_poly + f\"{cbsacode}_polygons.gpkg\", index = False)\n",
    "\n",
    "        plot_polygons(polygons, natbar, folder_plots, city_name)\n",
    "\n",
    "        del natbar, polygons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## postprocess charlotte - remove short rivers!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_name = \"Charlotte\"\n",
    "tag_dict = tags[city_name].copy()\n",
    "\n",
    "gdfs = []\n",
    "\n",
    "cbsacode = gdf_cbsacodes[gdf_cbsacodes[\"name\"]==city_name].cbsacode.values[0]\n",
    "print(cbsacode)\n",
    "\n",
    "natbar_path = folder_poly + f\"{cbsacode}_natbar.gpkg\"\n",
    "polygon_path = folder_poly + f\"{cbsacode}_polygons.gpkg\"\n",
    "\n",
    "# do this only if not done yet\n",
    "\n",
    "city_border = gpd.GeoDataFrame(\n",
    "    {\n",
    "        \"geometry\": [gdf_cbsacodes[gdf_cbsacodes[\"name\"]==city_name].boundary.values[0]],\n",
    "        \"barrier_type\": \"city_border\"\n",
    "    },\n",
    "    crs = gdf_cbsacodes.crs\n",
    ")\n",
    "\n",
    "gdfs.append(city_border)\n",
    "\n",
    "for barrier_type in barrier_types:\n",
    "    gdf = data_dict[city_name][barrier_type].copy()\n",
    "    gdf = gdf[(gdf.geom_type==\"LineString\")&(gdf[barrier_type]).isin(tag_dict[barrier_type])]\n",
    "    gdf[\"barrier_type\"] = gdf[barrier_type]\n",
    "    del gdf[barrier_type]\n",
    "    # for waterways, drop short pieces:\n",
    "    if barrier_type == \"waterway\":\n",
    "        print(len(gdf))\n",
    "        gdf_proj = gdf.to_crs(proj_crs).copy()\n",
    "        gdf_proj = gdf_proj[gdf_proj.length > 1000].copy()\n",
    "        gdf = gdf_proj.to_crs(gdf.crs)\n",
    "        print(len(gdf))\n",
    "    data_dict[city_name][barrier_type] = gdf\n",
    "    gdfs.append(gdf)\n",
    "\n",
    "    # now we have a list of 3 gdfs: city border, railway, waterway. concat them all:\n",
    "    # and save to file - these are the polygon outlines\n",
    "    natbar = pd.concat(gdfs).reset_index(drop=True)\n",
    "    natbar.to_file(folder_poly + f\"{cbsacode}_natbar.gpkg\", index = False)\n",
    "\n",
    "    # and polygonize & save\n",
    "    polygons = momepy.FaceArtifacts(natbar).polygons[[\"geometry\"]].set_crs(natbar.crs)\n",
    "    polygons.to_file(folder_poly + f\"{cbsacode}_polygons.gpkg\", index = False)\n",
    "\n",
    "    plot_polygons(polygons, natbar, folder_plots, city_name)\n",
    "\n",
    "    del natbar, polygons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reprocess orlando and miami - removing short canals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**orlando**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_name = \"Miami\"\n",
    "barrier_type = \"waterway\"\n",
    "cbsacode = gdf_cbsacodes[gdf_cbsacodes[\"name\"]==city_name].cbsacode.values[0]\n",
    "folder_osm = \"../data/natural_barriers/raw/\"\n",
    "barrier_path = folder_osm + f'{cbsacode}_{barrier_type}.gpkg'\n",
    "gdf = gpd.read_file(barrier_path)\n",
    "assert all([t in [\"Point\", \"LineString\", \"Polygon\"] for t in gdf.geom_type.unique()]), \"Unexpected geom type, double check\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gdf[gdf.geom_type==\"LineString\"]\n",
    "gdf = gdf[gdf[barrier_type].isin([\"canal\", \"river\", \"stream\", \"drain\"])].copy().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gdf.to_crs(proj_crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf[\"lengths\"] = gdf.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = gdf.explore(tiles = \"cartodb positron\", column = barrier_type, name = \"all\")\n",
    "folium.LayerControl().add_to(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.crs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try merging small polygons to their (also small) neighbours?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = gpd.read_file(f\"../data/natural_barriers/polygonized/33100_polygons.gpkg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* add area column\n",
    "* find reasonable threshold for merging adjacent polygons (contiguous! cf simp) that are too small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from esda.shape import diameter_ratio\n",
    "from libpysal import graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project\n",
    "poly = poly.to_crs(proj_crs)\n",
    "poly[\"sqkm\"] = poly.area / 10**6\n",
    "poly[\"i\"] = poly.geometry.apply(lambda x: diameter_ratio(x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# poly[poly.i<0.12].explore(column = \"i\", tiles = \"cartodb positron\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_merge = poly[(poly.i > 0.12)&(poly.sqkm<7)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#poly_merge.explore(tiles=\"cartodb positron\", column=\"sqkm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rook = graph.Graph.build_contiguity(poly_merge, rook=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_merge[\"label\"] = poly_merge.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_merge[\"neighbours\"] = poly_merge.label.apply(lambda x: rook.neighbors[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_merge[\"comp\"] = rook.component_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# poly_merge.explore(tiles=\"cartodb positron\", column=\"comp\", cmap = \"Dark2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually untangle: 642 & 643 are their own comp; also 701"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_comp = int(poly_merge.comp.max())\n",
    "poly_merge.loc[[642,643],\"comp\"] = last_comp + 1\n",
    "poly_merge.loc[701,\"comp\"] = last_comp + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use info to merge rest of polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly[\"merge\"] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ix, row in poly_merge.iterrows():\n",
    "    poly.loc[ix,\"merge\"] = row.comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly.loc[poly_merge.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these that have no particular comp to be merged with - keep as they are\n",
    "poly_keep = poly[poly[\"merge\"].isna()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these that need to be merged - merge them\n",
    "geoms = []\n",
    "for comp, mygroup in poly_merge.groupby(\"comp\"):\n",
    "    geoms.append(mygroup.geometry.union_all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_gdf = gpd.GeoDataFrame(\n",
    "    {\n",
    "        \"geometry\": list(poly_keep[\"geometry\"]) + geoms\n",
    "    }, \n",
    "    crs = poly.crs\n",
    ")\n",
    "new_gdf = new_gdf.to_crs(\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = new_gdf.explore(tiles=\"cartodb positron\", name = \"new\")\n",
    "poly.explore(m=m, name = \"old\", color = \"red\")\n",
    "folium.LayerControl().add_to(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "redo this but without dropping indeces -- then we can track which polygons to merge!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly.explore()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "highenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
